{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOokiH41WjatVSMZCvh0Ph+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmohsbeck1/jpmc_mle/blob/week-Apr.-11/JM_HyperParams_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-k4-RhjmiNo"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# John Mohsbeck\n",
        "# 4-11-2023\n",
        "# \n",
        "# Hyperparameter Optimization: Grid Search vs. Random Search vs. Bayesian Optimization in Action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/fenago/datasets/main/diamonds.csv\"\n",
        "diamonds = pd.read_csv(url)\n",
        "\n",
        "# Preprocessing\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds['cut'] = label_encoder.fit_transform(diamonds['cut'])\n",
        "diamonds['color'] = label_encoder.fit_transform(diamonds['color'])\n",
        "diamonds['clarity'] = label_encoder.fit_transform(diamonds['clarity'])\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X = diamonds.drop('cut', axis=1)\n",
        "y = diamonds['cut']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "yNpyQXTnmvDF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier model\n",
        "\n",
        "GridSearchCV"
      ],
      "metadata": {
        "id": "mJV8p-AVxUEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestClassifier model\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters to be tuned\n",
        "hyperparameters = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model on the training set\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters found by GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters found by GridSearchCV:\", best_params)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_score = grid_search.score(X_test, y_test)\n",
        "print(\"Test set accuracy with best hyperparameters:\", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P9qyUsqnnz7",
        "outputId": "99b76ab2-164f-46c5-f35e-e438619cda64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Best hyperparameters found by GridSearchCV: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "Test set accuracy with best hyperparameters: 0.7868001483129403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestRegressor model \n",
        "\n",
        "RandomizedSearchCV"
      ],
      "metadata": {
        "id": "Dcna2dUVxMgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/fenago/datasets/main/diamonds.csv\"\n",
        "diamonds = pd.read_csv(url)\n",
        "\n",
        "# Preprocessing\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds['cut'] = label_encoder.fit_transform(diamonds['cut'])\n",
        "diamonds['color'] = label_encoder.fit_transform(diamonds['color'])\n",
        "diamonds['clarity'] = label_encoder.fit_transform(diamonds['clarity'])\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X = diamonds.drop('price', axis=1)\n",
        "y = diamonds['price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "_a4yJKA7woOG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestRegressor model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Define hyperparameters to be tuned\n",
        "hyperparameters = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=hyperparameters, n_iter=10, cv=5, n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "# Fit the model on the training set\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters found by RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best hyperparameters found by RandomizedSearchCV:\", best_params)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_score = random_search.score(X_test, y_test)\n",
        "print(\"Test set R^2 score with best hyperparameters:\", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Rj3qjKxpm9",
        "outputId": "aa54ad6c-746e-433f-9045-edb52d26251a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best hyperparameters found by RandomizedSearchCV: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': 30}\n",
            "Test set R^2 score with best hyperparameters: 0.9999650617633047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Optimization\n",
        "\n",
        "XGBRegressor model"
      ],
      "metadata": {
        "id": "uxKUXymc2KAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ExEU3mCxQ8k",
        "outputId": "fe0cb65f-637f-4ace-b4d2-cf4091aa0a68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.9/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from bayesian-optimization) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Collecting colorama>=0.4.6\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.2 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xgboost as xgb\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/fenago/datasets/main/diamonds.csv\"\n",
        "diamonds = pd.read_csv(url)\n",
        "\n",
        "# Preprocessing\n",
        "label_encoder = LabelEncoder()\n",
        "diamonds['cut'] = label_encoder.fit_transform(diamonds['cut'])\n",
        "diamonds['color'] = label_encoder.fit_transform(diamonds['color'])\n",
        "diamonds['clarity'] = label_encoder.fit_transform(diamonds['clarity'])\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X = diamonds.drop('carat', axis=1)\n",
        "y = diamonds['carat']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "3efObMkK2Wqu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to optimize\n",
        "def xgb_cv(n_estimators, max_depth, gamma, min_child_weight, subsample, data, target):\n",
        "    estimator = xgb.XGBRegressor(\n",
        "        n_estimators=int(n_estimators),\n",
        "        max_depth=int(max_depth),\n",
        "        gamma=gamma,\n",
        "        min_child_weight=min_child_weight,\n",
        "        subsample=subsample,\n",
        "        random_state=42,\n",
        "    )\n",
        "    cval = cross_val_score(estimator, data, target, scoring='neg_mean_squared_error', cv=5)\n",
        "    return cval.mean()\n",
        "\n",
        "# BayesianOptimization\n",
        "def optimize_xgb(data, target):\n",
        "    def xgb_crossval(n_estimators, max_depth, gamma, min_child_weight, subsample):\n",
        "        return xgb_cv(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            gamma=gamma,\n",
        "            min_child_weight=min_child_weight,\n",
        "            subsample=subsample,\n",
        "            data=data,\n",
        "            target=target,\n",
        "        )\n",
        "\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=xgb_crossval,\n",
        "        pbounds={\n",
        "            \"n_estimators\": (50, 500),\n",
        "            \"max_depth\": (3, 10),\n",
        "            \"gamma\": (0, 1),\n",
        "            \"min_child_weight\": (0, 10),\n",
        "            \"subsample\": (0.5, 1),\n",
        "        },\n",
        "        random_state=42,\n",
        "        verbose=2,\n",
        "    )\n",
        "    optimizer.maximize(init_points=5, n_iter=10)\n",
        "    return optimizer.max"
      ],
      "metadata": {
        "id": "eN6qWLQh2dGf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Find optimal hyperparameters using Bayesian Optimization\n",
        "best_params = optimize_xgb(X_train, y_train)\n",
        "print(\"Best hyperparameters found by Bayesian Optimization:\", best_params)\n",
        "\n",
        "# Train the XGBoost model with the best hyperparameters\n",
        "best_xgb = xgb.XGBRegressor(\n",
        "    n_estimators=int(best_params[\"params\"][\"n_estimators\"]),\n",
        "    max_depth=int(best_params[\"params\"][\"max_depth\"]),\n",
        "    gamma=best_params[\"params\"][\"gamma\"],\n",
        "    min_child_weight=best_params[\"params\"][\"min_child_weight\"],\n",
        "    subsample=best_params[\"params\"][\"subsample\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test set Mean Squared Error with best hyperparameters:\", mse)\n",
        "print(\"Test set R^2 score with best hyperparameters:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24cC5qUK3icl",
        "outputId": "c6df1cc0-483f-4037-c201-d6a1c56f2e6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   gamma   | max_depth | min_ch... | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-0.000653\u001b[0m | \u001b[0m0.3745   \u001b[0m | \u001b[0m9.655    \u001b[0m | \u001b[0m7.32     \u001b[0m | \u001b[0m319.4    \u001b[0m | \u001b[0m0.578    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-0.000715\u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m3.407    \u001b[0m | \u001b[0m8.662    \u001b[0m | \u001b[0m320.5    \u001b[0m | \u001b[0m0.854    \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m-0.000400\u001b[0m | \u001b[95m0.02058  \u001b[0m | \u001b[95m9.789    \u001b[0m | \u001b[95m8.324    \u001b[0m | \u001b[95m145.6    \u001b[0m | \u001b[95m0.5909   \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-0.000543\u001b[0m | \u001b[0m0.1834   \u001b[0m | \u001b[0m5.13     \u001b[0m | \u001b[0m5.248    \u001b[0m | \u001b[0m244.4    \u001b[0m | \u001b[0m0.6456   \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-0.000813\u001b[0m | \u001b[0m0.6119   \u001b[0m | \u001b[0m3.976    \u001b[0m | \u001b[0m2.921    \u001b[0m | \u001b[0m214.9    \u001b[0m | \u001b[0m0.728    \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m-0.000394\u001b[0m | \u001b[95m0.02288  \u001b[0m | \u001b[95m6.018    \u001b[0m | \u001b[95m4.267    \u001b[0m | \u001b[95m133.5    \u001b[0m | \u001b[95m0.892    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-0.000680\u001b[0m | \u001b[0m0.7196   \u001b[0m | \u001b[0m9.834    \u001b[0m | \u001b[0m9.851    \u001b[0m | \u001b[0m106.8    \u001b[0m | \u001b[0m0.8844   \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-0.000725\u001b[0m | \u001b[0m0.1771   \u001b[0m | \u001b[0m3.181    \u001b[0m | \u001b[0m0.3958   \u001b[0m | \u001b[0m159.5    \u001b[0m | \u001b[0m0.8779   \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-0.000664\u001b[0m | \u001b[0m0.5916   \u001b[0m | \u001b[0m9.997    \u001b[0m | \u001b[0m8.093    \u001b[0m | \u001b[0m146.2    \u001b[0m | \u001b[0m0.9629   \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-0.000730\u001b[0m | \u001b[0m0.8076   \u001b[0m | \u001b[0m4.231    \u001b[0m | \u001b[0m3.167    \u001b[0m | \u001b[0m171.0    \u001b[0m | \u001b[0m0.8629   \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-0.000705\u001b[0m | \u001b[0m0.4913   \u001b[0m | \u001b[0m4.79     \u001b[0m | \u001b[0m7.352    \u001b[0m | \u001b[0m318.1    \u001b[0m | \u001b[0m0.899    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-0.000441\u001b[0m | \u001b[0m0.07546  \u001b[0m | \u001b[0m8.683    \u001b[0m | \u001b[0m4.667    \u001b[0m | \u001b[0m187.6    \u001b[0m | \u001b[0m0.6494   \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-0.000415\u001b[0m | \u001b[0m0.0825   \u001b[0m | \u001b[0m9.327    \u001b[0m | \u001b[0m5.937    \u001b[0m | \u001b[0m387.4    \u001b[0m | \u001b[0m0.874    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-0.000709\u001b[0m | \u001b[0m0.7281   \u001b[0m | \u001b[0m7.672    \u001b[0m | \u001b[0m9.945    \u001b[0m | \u001b[0m233.0    \u001b[0m | \u001b[0m0.7694   \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-0.000463\u001b[0m | \u001b[0m0.1336   \u001b[0m | \u001b[0m8.161    \u001b[0m | \u001b[0m3.013    \u001b[0m | \u001b[0m135.3    \u001b[0m | \u001b[0m0.6134   \u001b[0m |\n",
            "=====================================================================================\n",
            "Best hyperparameters found by Bayesian Optimization: {'target': -0.00039476840782782076, 'params': {'gamma': 0.022881274019683873, 'max_depth': 6.018152538177446, 'min_child_weight': 4.267257798733937, 'n_estimators': 133.4954300350788, 'subsample': 0.8920095268856046}}\n",
            "Test set Mean Squared Error with best hyperparameters: 0.00031088676716785205\n",
            "Test set R^2 score with best hyperparameters: 0.9986317973754205\n"
          ]
        }
      ]
    }
  ]
}